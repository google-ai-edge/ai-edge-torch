# Copyright 2025 The AI Edge Torch Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Numerical validation tests for torch ops and Torch-TFL ops."""

from typing import Any, Dict, Sequence

import ai_edge_torch
from ai_edge_torch import testing
from ai_edge_torch.odml_torch.experimental import torch_tfl
import numpy as np
import torch
from torch.utils import _pytree as pytree

from absl.testing import absltest as googletest
from absl.testing import parameterized


export_with_tensor_inputs_only = testing.export_with_tensor_inputs_only


def rnd(dtype, shape, min_v=None, max_v=None):
  """Shortcut for creating a random torch tensor."""
  if dtype in (torch.int32, torch.int64, torch.bool):
    min_v = min_v if min_v else 1
    max_v = max_v if max_v else 10
    return torch.randint(min_v, max_v, shape).to(dtype)
  else:
    min_v = min_v if min_v else 0.0
    max_v = max_v if max_v else 1.0
    return (torch.rand(shape) * (max_v - min_v) + min_v).to(dtype)


class TestTorchTFLImpls(parameterized.TestCase):
  """Numerical validation tests for torch ops and Torch-TFL ops.

  The op test suite is forked from
  ai_edge_torch/odml_torch/test/test_core_aten_ops.py. Eventually, we should
  merge the two test suites.
  """

  def setUp(self):
    super().setUp()
    torch.manual_seed(0)

  def _assert_export_and_close(
      self,
      func,
      args,
      kwargs,
      dynamic_shapes: Dict[str, Any] | Sequence[Any] | None = None,
      atol=1e-3,
      rtol=1e-5,
      equal_nan=True,
  ):
    """Assert func, args, and kwargs can be lowered and pass numerical validation."""
    with self.subTest("torch_eval"):
      expected = func(*args, **kwargs)

      with self.subTest("export_and_decompse"):
        exported_program = export_with_tensor_inputs_only(
            func, args, kwargs, dynamic_shapes
        )
        exported_program = exported_program.run_decompositions(
            torch_tfl.decomps
        )

        with self.subTest("decomp_eval"):
          args, kwargs = exported_program.example_inputs
          actual = exported_program.module()(*args, **kwargs)

          with self.subTest("torch_lower_eval_diff:" + str(atol)):
            expected_flat, expected_spec = pytree.tree_flatten(expected)
            actual_flat, actual_spec = pytree.tree_flatten(actual)

            self.assertEqual(expected_spec, actual_spec)
            for v1, v2 in zip(expected_flat, actual_flat):
              torch.testing.assert_close(
                  v1, v2, atol=atol, rtol=rtol, equal_nan=equal_nan
              )

        with self.subTest("convert_eval"):
          args, kwargs = exported_program.example_inputs
          edge_model = ai_edge_torch.convert(
              exported_program.module(), args, dynamic_shapes=dynamic_shapes
          )
          actual = edge_model(*args, **kwargs)

          with self.subTest("torch_convert_eval_diff:" + str(atol)):
            expected_flat, expected_spec = pytree.tree_flatten(expected)
            actual_flat, actual_spec = pytree.tree_flatten(actual)

            self.assertEqual(expected_spec, actual_spec)
            for v1, v2 in zip(expected_flat, actual_flat):
              # Convert NumPy arrays to PyTorch tensors if necessary
              if isinstance(v1, torch.Tensor) and isinstance(v2, np.ndarray):
                v2 = torch.from_numpy(v2)
              torch.testing.assert_close(
                  v1, v2, atol=atol, rtol=rtol, equal_nan=equal_nan
              )

  @parameterized.named_parameters(
      # fmt: off
      # pyformat: disabledef
      ("aten_mm_0", torch.ops.aten.mm.default, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_mm_1", torch.ops.aten.mm.default, (rnd(torch.float32, (2, 10)), rnd(torch.float32, (10, 5)),), dict()),
      ("aten_add_Tensor_0", torch.ops.aten.add.Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_add_Tensor_1", torch.ops.aten.add.Tensor, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict(alpha=10)),
      ("aten_add_Tensor_2", torch.ops.aten.add.Tensor, (rnd(torch.float32, (10, 10)), np.random.rand(),), dict()),
      ("aten_sub_Tensor_0", torch.ops.aten.sub.Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_sub_Tensor_1", torch.ops.aten.sub.Tensor, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict(alpha=10)),
      ("aten_sub_Tensor_2", torch.ops.aten.sub.Tensor, (rnd(torch.float32, (10, 10)), np.random.rand(),), dict()),
      ("aten_mul_Tensor_0", torch.ops.aten.mul.Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_mul_Tensor_1", torch.ops.aten.mul.Tensor, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict()),
      ("aten_mul_Tensor_2", torch.ops.aten.mul.Tensor, (rnd(torch.float32, (10, 10)), np.random.rand(),), dict()),
      ("aten_mul_Scalar_0", torch.ops.aten.mul.Scalar, (rnd(torch.float32, (10, 10)), np.random.rand(),), dict()),
      ("aten_div_Tensor_0", torch.ops.aten.div.Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_div_Tensor_1", torch.ops.aten.div.Tensor, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict()),
      ("aten_div_Tensor_2", torch.ops.aten.div.Tensor, (rnd(torch.float32, (10, 10)), np.random.rand(),), dict()),
      ("aten_pow_Scalar_0", torch.ops.aten.pow.Scalar, (np.random.rand(), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_pow_Tensor_Scalar_0", torch.ops.aten.pow.Tensor_Scalar, (rnd(torch.float32, (10, 10)), np.random.rand(),), dict()),
      ("aten_pow_Tensor_Tensor_0", torch.ops.aten.pow.Tensor_Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_gt_Tensor_0", torch.ops.aten.gt.Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_gt_Tensor_1", torch.ops.aten.gt.Tensor, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict()),
      ("aten_lt_Tensor_0", torch.ops.aten.lt.Tensor, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_lt_Tensor_1", torch.ops.aten.lt.Tensor, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict()),
      ("aten_maximum_0", torch.ops.aten.maximum.default, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_maximum_1", torch.ops.aten.maximum.default, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict()),
      ("aten_minimum_0", torch.ops.aten.minimum.default, (rnd(torch.float32, (10, 10)), rnd(torch.float32, (10, 10)),), dict()),
      ("aten_minimum_1", torch.ops.aten.minimum.default, (rnd(torch.float32, (1, 10)), rnd(torch.float32, (10, 1)),), dict()),
      ("aten_sin_0", torch.ops.aten.sin.default, (rnd(torch.float32, (10, 10)),), dict()),
      ("aten_sin_1", torch.ops.aten.sin.default, (rnd(torch.float32, (1, 10)),), dict()),
      ("aten_cos_0", torch.ops.aten.cos.default, (rnd(torch.float32, (10, 10)),), dict()),
      ("aten_cos_1", torch.ops.aten.cos.default, (rnd(torch.float32, (1, 10)),), dict()),
      ("aten_rsqrt_0", torch.ops.aten.rsqrt.default, (rnd(torch.float32, (10, 10)),), dict()),
      ("aten_rsqrt_1", torch.ops.aten.rsqrt.default, (rnd(torch.float32, (1, 10)),), dict()),
      ("aten_gelu_0", torch.ops.aten.gelu.default, (rnd(torch.float32, (10, 10)),), dict()),
      ("aten_gelu_1", torch.ops.aten.gelu.default, (rnd(torch.float32, (10, 10)),), dict(approximate="tanh")),
      ("aten_gelu_2", torch.ops.aten.gelu.default, (rnd(torch.float32, (1, 10)),), dict()),
      ("aten_gelu_3", torch.ops.aten.gelu.default, (rnd(torch.float32, (1, 10)),), dict(approximate="tanh")),
      ("aten_permute_0", torch.ops.aten.permute.default, (rnd(torch.float32, (10, 10)), [0, 1],), dict()),
      ("aten_permute_1", torch.ops.aten.permute.default, (rnd(torch.float32, (1, 10)), [0, 1],), dict()),
      ("aten_view_0", torch.ops.aten.view.default, (rnd(torch.float32, (10, 10)), [1, 100],), dict()),
      ("aten_view_1", torch.ops.aten.view.default, (rnd(torch.float32, (1, 10)), [10, 1],), dict()),
      ("aten_view_2", torch.ops.aten.view.default, (rnd(torch.float32, (10, 10)), [2, 5, 10],), dict()),
      ("aten_view_3", torch.ops.aten.view.default, (rnd(torch.float32, (10, 10)), [1, -1],), dict()),
      ("aten_view_4", torch.ops.aten.view.default, (rnd(torch.float32, (10, 10)), [2, -1, 10],), dict()),
      ("aten_view_5", torch.ops.aten.view.default, (rnd(torch.float32, (10, 10)), [-1, 2, 10],), dict()),
      ("aten__softmax_0", torch.ops.aten._softmax.default, (rnd(torch.float32, (10, 10)), -1, False), dict()),
      ("aten__softmax_1", torch.ops.aten._softmax.default, (rnd(torch.float32, (1, 10)), -1, False), dict()),
      ("aten__softmax_2", torch.ops.aten._softmax.default, (rnd(torch.float32, (10, 10)), 0, False), dict()),
      ("aten__softmax_3", torch.ops.aten._softmax.default, (rnd(torch.float32, (1, 10)), 0, False), dict()),
      ("aten__softmax_4", torch.ops.aten._softmax.default, (rnd(torch.float32, (10, 10)), 1, False), dict()),
      ("aten__softmax_5", torch.ops.aten._softmax.default, (rnd(torch.float32, (1, 10)), 1, False), dict()),
      # fmt: on
      # pyformat: enable
  )
  def test_op(
      self,
      op,
      args,
      kwargs,
  ):
    self._assert_export_and_close(op, args, kwargs)

  @parameterized.named_parameters(
      # fmt: off
      # pyformat: disabledef
      ("reshape_without_dynamic_shape_0", (rnd(torch.float32, (10, 2, 3)),), dict(), None),
      ("reshape_with_dynamic_shape_1", (rnd(torch.float32, (10, 2, 3)),), dict(), ((torch.export.Dim("batch"), None, None),)),
      ("reshape_with_dynamic_shape_2", (rnd(torch.float32, (10, 2, 3)),), dict(), ({0: torch.export.Dim("batch")},)),
      ("reshape_with_dynamic_shape_3", (rnd(torch.float32, (10, 2, 3)),), dict(), ((torch.export.Dim("batch"), torch.export.Dim("height"), torch.export.Dim("width")),)),
      ("reshape_with_dynamic_shape_4", (rnd(torch.float32, (10, 2, 3)),), dict(), ({0: torch.export.Dim("batch"), 1: torch.export.Dim("height"), 2: torch.export.Dim("width")},)),
      # fmt: on
      # pyformat: enable
  )
  def test_reshape_op(
      self,
      args,
      kwargs,
      dynamic_shapes: Dict[str, Any] | Sequence[Any] | None = None,
  ):

    class ReshapeModel(torch.nn.Module):

      def forward(self, x):
        x = x + x
        x = x.reshape([x.shape[0], x.shape[1] * x.shape[2]])
        return x

    self._assert_export_and_close(ReshapeModel(), args, kwargs, dynamic_shapes)


if __name__ == "__main__":
  googletest.main()
